# 【42 锚框】动手学深度学习v2｜学习笔记

 **Author:** [修仙]

 **Link:** [https://zhuanlan.zhihu.com/p/455807888]

锚框这一讲感觉还是蛮绕的(而且视频里面对大部分代码都是一笔带过)，整个看下来打开花了2-3天的时间，参考了许多其他博客，记录如下。

深度学习目标检测算法分成基于锚框的和无锚框的，如下图所示

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-cac2e1dc869bef4a852eb1ba21251c8e_r.jpg)
> 目标检测算法通常会在输入图像中**采样大量的区域**，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的**真实边界框**(ground-truth)。 不同的模型使用的区域采样方法可能不同。 这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比(aspect ratio)不同的边界框。 这些边界框被称为锚框(anchor box)。 ——《动手学深度学习v2》

看了上面的介绍，可能还是不知道锚框是什么，下面先**简要概述**一下：在一个完整的目标检测流程中应该怎样使用锚框。

1.理论介绍
------

### **1.1 打标签**(给锚框打标签)

这里的打标签和我们以往人工对gt打标签不同，它是在人工打好gt标签的基础上，为anchor自动生成标签。

* 首先生成锚框(以每个像素点为单位生成，例如：可以通过不同的大小比例、宽高比来为每个锚框生成多个anchor)
* 接着，给每个锚框匹配gt(IOU小于阈值的锚框直接划分为背景)，匹配完成之后，锚框就相当于被打上了标签，标签为锚框的类别和偏移量

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-2ed33f0cbb3fb2f4730923a6a99cfa3b_r.jpg)

### 1.2 训练

图像经过网络后生成一系列的Anchor，接着就计算它们和真实Anchor的Loss，经过反向传播学习更新参数，使得L最小。

(参考[Jackpop：锚框：Anchor box综述](https://zhuanlan.zhihu.com/p/63024247))，看一下**损失函数**会更加有助于理解这个概念，Faster R-CNN原文中采用的损失函数为：

![image-20230511102207946](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/image-20230511102207946.png)

$L_{c l s}\left(p_{i}, p_{i}^{*}\right)$是类别损失， p^*_i是真实标签，如果是正样本，标注为1,如果是负样本标注为0(Faster R-CNN网络的产生的anchor只分为前景和背景，前景的标签为1，背景的标签为0)。同理， L_{r e g}\left(t_{i}, t_{i}^{*}\right) 为位置偏移损失， t^*_i 是真实边框相对于anchor box的4个参数化坐标的向量，训练的最终目标就是使得损失函数 L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right) 最小化，这样会得到使得预测类别 p_i 、预测偏移量 t_i 与真实标注最佳拟合模型参数。

### 1.3 测试

运用训练好的网络生成多个Anchor box(类别和偏移量组成，这样可以反向推出预测框的坐标)，最后运用NMS去除多余的预测框。



---

2.代码实现
------

下面结合代码，完整的叙述目标检测中锚框的使用。

### 2.1 生成多个锚框

假设输入图像的高度为ℎ，宽度为w。以图像的每个像素为中心生成不同形状的锚框：比例为 s ∈(0,1]，宽高比为 r>0 。要生成多个不同形状的锚框，设置一系列刻度s_{1}, ...,s_{n} 和一系列宽高比r_{1}, ...,r_{m}。为了**减少计算复杂度**，只考虑包含s_{1}或r_{1}的组合，如下图所示。

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-645dd54057e01a7debca982c0dd9b599_r.jpg)对于锚框高宽的求法和视频里讲解的不太一样(感觉视频中的难以理解，并且视频讲义中设置的变量 r是原图的高框比/锚框的高宽比，用起来比较变扭，所以采用如下的方式来写)

设 w, h  为 图像中锚框的实际宽高，  W，H 为图像的宽高

由 \frac{wh}{WH} = s^{2}  和 \frac{w}{h} = r \Rightarrow w = s\sqrt{HWr} 和 h = s \sqrt{\frac{HW}{r}} 

对 w 和 h **归一化** \Rightarrow**w_{0} = \frac{w}{W} = s \sqrt{\frac{Hr}{W}}** 和 **h_{0} = \frac{h}{H} = s \sqrt{\frac{W}{Hr}}** 

显然，当 H = W 时， w_{0} = s \sqrt{r}, h_{0} = \frac{s}{\sqrt{r}}

首先，导包，并且构造**函数的输入**，进而一步步看函数到底生成了什么


```python
import torch

data = torch.rand(size=(1, 3, 100, 80)) # 构造输⼊数据
sizes=[0.75, 0.5, 0.25]
ratios=[1, 2, 0.5]
```
接着开始写函数，为了方便理解，我们把函数一块块分下来看。

首先，取出data中的后两位，也就是图像的高宽；接着把list格式的size和ratio都转为torch格式


```python
def multibox_prior(data, sizes, ratios):
    """生成以每个像素为中心具有不同形状的锚框"""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios) # 3, 3
    boxes_per_pixel = (num_sizes + num_ratios - 1) # 每个像素的锚框数，一共有w*h*boxes_per_pixel个锚框
    size_tensor = torch.tensor(sizes, device=device) # list 转为 tensor
    ratio_tensor = torch.tensor(ratios, device=device)
```
 为了将锚点移动到像素的中心，需要设置偏移量。


```python
    # 因为一个像素的的高为1且宽为1，我们选择偏移我们的中心0.5
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # 在y轴上缩放步长，在上面举的例子里面 steps_h=1/100=0.01
    steps_w = 1.0 / in_width  # 在x轴上缩放步长 steps_w=1/80=0.0125
    # 这里是归一化
```
下面通过torch.meshgrid()函数**生成锚框的所有中心点**


```python
    # 生成锚框的所有中心点
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    shift_y, shift_x = torch.meshgrid(center_h, center_w)
#由于上面的例子中center_h和center_w的个数太多了分别有100和80个
#这边举一个简单的例子 来看一下torch.meshgrid()函数的工作原理
    # center_h： tensor([0.1250, 0.3750, 0.6250, 0.8750])
    # center_w： tensor([0.1250, 0.3750, 0.6250, 0.8750])
    # shift_y tensor([
    # [0.1250, 0.1250, 0.1250, 0.1250],
    # [0.3750, 0.3750, 0.3750, 0.3750],
    # [0.6250, 0.6250, 0.6250, 0.6250],
    # [0.8750, 0.8750, 0.8750, 0.8750]]) 
        
    # shift_x tensor([
    # [0.1250, 0.3750, 0.6250, 0.8750],
    # [0.1250, 0.3750, 0.6250, 0.8750],
    # [0.1250, 0.3750, 0.6250, 0.8750],
    # [0.1250, 0.3750, 0.6250, 0.8750]])
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1) #展平
    # shift_y和shift_x是每个锚框的中心点坐标，现在将其展平，一一对应的就是每个点坐标
    # 全部每个像素中心点坐标
    # tensor([0.1250, 0.1250, 0.1250, 0.1250, 0.3750, 0.3750, 0.3750, 0.3750, 0.6250,
    # 0.6250, 0.6250, 0.6250, 0.8750, 0.8750, 0.8750, 0.8750]) 
    # tensor([0.1250, 0.3750, 0.6250, 0.8750, 0.1250, 0.3750, 0.6250, 0.8750, 0.1250,
    # 0.3750, 0.6250, 0.8750, 0.1250, 0.3750, 0.6250, 0.8750])

```
为每个像素点生成“boxes_per_pixel”个锚框的高和宽


```python
    # 生成“boxes_per_pixel”个高和宽
    # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)

    w_0 = torch.cat((sizes[0] * torch.sqrt(in_height * ratio_tensor[:] / in_width),
                     size_tensor[1:] * torch.sqrt(in_height * ratio_tensor[0] / in_width)))
    h_0 = torch.cat((sizes[0] * torch.sqrt(in_width / ratio_tensor[:] / in_height), 
                     size_tensor[1:] * torch.sqrt(in_width / ratio_tensor[0] / in_height)))
    #创建的是s+r-1个锚框，cat是将两种情况联合：一种是s变r不变，一种是s不变r变
    
    # 除以2来获得半高和半宽(半高半宽和中心坐标相加后，就得到左上角和右下角的坐标)
    #原本每个w_0的shape是[1，5], repeat 8000次。
    #这边.T.repeat的具体用法没查到 T.repeat(,),应该会第一维度是重复行，第二维度是重复列
    #所以最后整个anchor_manipulations的shpe就是[8000*5,4]
    anchor_manipulations = torch.stack((-w_0, -h_0, w_0, h_0)).T.repeat(
                                        in_height * in_width, 1) / 2
    # stack后，(-w_0, -h_0, w_0, h_0)生成的shape就是4*5，这个5就是s+r-1
    # 接着.T转置，转成的shape是5*4
    # 然后repeat，有两个参数，第一个是指dim为0的方向重复多少次，第二个是dim为1的方向重复多少次
    # 最后的shape就是（5*8000）*4
    # 最后除以2，因为要把这个变量和锚框的中心点相加，便可以得到锚框的左上角和右下角坐标
```
我们有了中心坐标和锚框的宽高，将两者相加得到左上角和右下角的坐标来表示所有锚框(一共有8000*5个锚框)，最终在output前面增加一个batch_size的维度，得到[1,40000,4]的输出。


```python
    # 每个中心点都将有“boxes_per_pixel”个锚框
    # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)
```
我们可以通过以下代码把输出的形状reshape一下，输出图像中任意一个像素点5个锚框中的1个


```python
boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]
#前两个代表左上角坐标，后两个是右下角坐标
#print(0.06,0.07,0.63,0.82)
```
### 2.2 可视化锚框

定义了下面的`show_bboxes`函数来显示以图像中以某个像素为中心的所有锚框.


```python
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """显示所有边界框"""
    def _make_list(obj, default_values=None):
        # 返回的是颜色、标签，如果没有的话，就返回默认的列表
        if obj is None:  #如果参数里面没给obj，就用后面default_values指定的，例如下面的color就是这个意思
            obj = default_values
		#这边if isinstance用来判断obj的类型是不是我们制定的
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = _make_list(labels)
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)
        axes.add_patch(rect)
        #下面的是用来进入text文字描述的
        if labels and len(labels) > i:
            text_color = 'k' if color == 'w' else 'w'
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))
```
因为我们在前面的处理中对锚框的宽高进行了归一化，所以绘制锚框时，我们需要恢复它们原始的坐标值。 因此，定义了变量`bbox_scale`。


```python
d2l.set_figsize()
bbox_scale = torch.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])
```
![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-cd7dd812e4bcd9398ecbb58fb4a71eef_r.jpg)### 2.3 交并比

在给锚框分配gt之前，先学习**交并比IOU**的概念和写法

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-77c382f69f5672964124be509ad7d2eb_r.jpg)
```python
def box_iou(boxes1, boxes2):
    """计算两个锚框或边界框列表中成对的交并比"""
    #boxes1(左上角x，左上角y，右下角x，右下角y)
#这边举个例子：boxes1 [1,1,3,3],[0,0,2,4],[1,2,3,4]] ；boxes2[[0,0,3,3],[2,0,5,2]]
    # 首先计算一个框的面积（长X宽）
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # 下面是将锚框和边界框作比较
    # 重叠部分左上角坐标（取最大的值）
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
#boxes1[:, None, :2]：5*1*2
# None的意思是在插入第二个维度
#boxes2[:, :2]:2*2
#由于它们的维度不同，所以要用广播机制，真正计算的时候，是下面这样的
#boxes1[:, None, :2]：5*2*2
#boxes2[:, :2]:2*2
#此时inter_upperlefts 为：5*2*2
    # 重叠部分右下角坐标（取最小的值，因为是在下面）
	inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
	#clamp(min=0)用来限制inters最小不能低于0
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)
    # 这个inter_areas是boxes1和boxes2重叠的部分
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas#这边又用了一次广播机制
    return inter_areas / union_areas
```
### 2.4 在训练数据中标注锚框

算法原理

**2.4.1 将真实边界框分配给锚框**

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-67ac0a313bc011dac9929c7495fdf51e_r.jpg)具体例子以及代码实现(这边的代码和教材里给的不太一样，教材里的也能实现，但是这里的写法和上面的算法原理完全对应，所以采用下面的写法，看起来比较清楚)


```python
ground_truth = torch.tensor([[0.1, 0.08, 0.52, 0.92],
                         [ 0.55, 0.2, 0.9, 0.88]])
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]])

def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """将最接近的真实边界框分配给锚框。"""
    # 锚框数量和真实边界框数量
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0] #在例子中是5个anchors和2个gt
    # 位于第i行和第j列的元素 x_ij 是锚框i和真实边界框j的IoU
    jaccard = box_iou(anchors, ground_truth) # jaccard是每个锚框的IoU
    """
	tensor([[0.0536, 0.0000],
	[0.1417, 0.0000],
	[0.0000, 0.5657],
	[0.0000, 0.2059],
	[0.0000, 0.7459]])
	"""
    # 定义anchors_bbox_map来记录anchor分别对应着什么gt，anchors_bbox_map存放标签初始全为-1
    #tensor([-1, -1, -1, -1, -1])
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,device=device)
    # 先为每个bb分配一个anchor(不要求满足iou_threshold，因为每个bb总得有个anchor)
    jaccard_cp = jaccard.clone()
    # 将最大元素的行和列用-1代替，相当于丢弃这行这列的所有元素
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)
    # 先遍历每一个真实边界框，为它们找到交并比最大的那个锚框
    for _ in range(num_gt_boxes):
        # 获取数值最大的那个元素的索引
        max_idx = torch.argmax(jaccard_cp) # 返回的是索引，相当于是先将jaccard_cp展平，然后返回索引，所以这个索引需要经过二次计算才能得到它在矩阵中的位置
        # 列索引
        box_idx = (max_idx % num_gt_boxes).long()
        # 行索引 锚框的下标
        anc_idx = (max_idx / num_gt_boxes).long()
        # 将真实边界框分配给锚框
        anchors_bbox_map[anc_idx] = box_idx
        # 把anc_idx行box_idx列元素变为-1，视为删掉
        jaccard_cp[:, box_idx] = col_discard
        jaccard_cp[anc_idx, :] = row_discard

    # 遍历剩余的na−nb个锚框
    # 处理还未被分配的anchor, 要求满足iou_threshold
    for i in range(num_anchors):
        # 索引等于初始值-1 的就是剩下的锚框
        if anchors_bbox_map[i] == -1:
            j = torch.argmax(jaccard[i, :])
            # 根据阈值，决定是否分配真实边界框
            if jaccard[i, j] >= iou_threshold:
                anchors_bbox_map[i] = j

    # 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1
    return anchors_bbox_map
```
我们例子的输出为：tensor([-1,0,1,-1,1])

**2.4.2 标记类别和偏移量**

算法原理

![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-56b45da98c8e24e8c0b0ba0312ee9dca_r.jpg)**计算偏移量**


```python
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """对锚框偏移量的转换。"""
    # 坐标转换 从（左上，右下）转换到（中间，宽度，高度）
    c_anc = box_corner_to_center(anchors) # 锚框坐标
    c_assigned_bb = box_corner_to_center(assigned_bb) # 真实边界框坐标
    # 偏移量计算公式
    #除0.2和0.1就是*10和*5
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    # 拼接
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset

#补充两个函数（左右到中间；中间到左右）
def box_corner_to_center(boxes):
    """从（左上, 右下）转换到(中间, 宽度, 高度)"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes

def box_center_to_corner(boxes):
    """从（中间, 宽度, 高度）转换到（左上, 右下）"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```
**标记锚框的类和偏移量**


```python
# anchors输入的锚框[1,锚框总数，4] labels真实标签[bn,真实锚框数，5]
#在下面的例子中为5，gt数目为2
‘’‘
labels = torch.tensor([[[0, 0.1, 0.08,	 0.52, 0.92],
                         [1, 0.55, 0.2, 0.9, 0.88]]])
anchors = torch.tensor([[[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]]])
‘’‘
def multibox_target(anchors, labels):
    """使用真实边界框标记锚框。"""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    # squeeze的作用是去除每个值为1的维度
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0] #5
    # 处理每个batch
    for i in range(batch_size):
        # 真实边界框
        label = labels[i, :, :]
        # 为每个锚框分配真实的边界框
        # assign_anchor_to_bbox函数返回，每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1
        # tensor([-1, 0, 1, -1, 1])
        #这边label[:, 1:] 从1开始是因为，求IOU的时候不需要用到类别
        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors, device)
        # label中的label[:,0]是类别参数，为0则是背景类别，为1是正类
```
下面求 bbox_mask(是为了和原始偏移量相乘，过滤掉背景的)


```python
        # bbox_mask: (锚框总数, 4), 0代表背景, 1代表非背景
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)
        #anchors_bbox_map >= 0得到的是True、False向量，通过float转为数字
        # unsqueeze将向量从(5,)扩展为(5,1)
        #没有repeat之前
        #tensor([[0.],
        # [1.],
        # [1.],
        # [0.],
        # [1.]])
        #repeat之后
        #tensor([[0., 0., 0., 0.],
      # [1., 1., 1., 1.],
      # [1., 1., 1., 1.],
      # [0., 0., 0., 0.],
      # [1., 1., 1., 1.]])
        # 将类标签和分配的边界框坐标初始化为零，tensor([0, 0, 0, 0, 0])，数量为锚框总数
        class_labels = torch.zeros(num_anchors, dtype=torch.long,device=device)
        # 所有anchor对应的真实边框坐标
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,device=device)
        #tensor([[0., 0., 0., 0.],
        #[0., 0., 0., 0.],
        #[0., 0., 0., 0.],
        #[0., 0., 0., 0.],
        #[0., 0., 0., 0.]])
        # 如果一个锚框没有被分配，我们标记其为背景（值为零）
        indices_true = torch.nonzero(anchors_bbox_map >= 0) # 获得的是非背景的索引 [-1, 0, 1, -1, 1]->[F,T,T,F,T]-> 1,2,4
        # 非背景对应的类别标签索引 0,1,1
        bb_idx = anchors_bbox_map[indices_true]
        # 背景为0，新类的整数索引递增1
        #class_lable为[0, 1, 2, 0, 2]
        # label是真实边界框，label[:,0]是类别参数
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        #把真实标注好的边界框的坐标值赋给与其对应的某一锚框，
        assigned_bb[indices_true] = label[bb_idx, 1:]	#给的是边界框的坐标
        # 偏移量转换，bbox_mask过滤掉背景
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
#tensor([-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, 1.40e+00, 1.00e+01,
# 2.59e+00, 7.18e+00, -1.20e+00, 2.69e-01, 1.68e+00, -1.57e+00,
# -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00,
# 4.17e-06, 6.26e-01])
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)

    """
 Returns:
 列表, [bbox_offset, bbox_mask, class_labels]
 bbox_offset: 每个锚框的标注偏移量
 bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1
 			shape为锚框数量×4
 cls_labels: 每个锚框的标注类别, 其中0表示为背景，shape为锚框数量
 """
    return (bbox_offset, bbox_mask, class_labels)
```
### 2.5 预测

 **根据带有预测偏移量的锚框来预测边界框**


```python
#预测的时候anchors是已知的，offset_preds是预测出来的
# 该函数将锚框和偏移量预测作为输入，并应用逆偏移变换来返回预测的边界框坐标。
def offset_inverse(anchors, offset_preds):
    """根据带有预测偏移量的锚框来预测边界框。"""
    # 从（左上，右下）转换到（中间，宽度，高度）
    anc = box_corner_to_center(anchors)
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)
    # 从（中间，宽度，高度）转换到（左上，右下）
    predicted_bbox = box_center_to_corner(pred_bbox)
    return predicted_bbox
```
**非极大值抑制函数**


```python
# 按降序对置信度进行排序并返回其索引
def nms(bboxs, scores, threshold):
    # 取出分数从大到小排列的索引 order为排序后的得分对应的原数组索引值
    order = torch.argsort(scores, dim=-1, descending=True)。 #[0,3,1,2]
    # 这边的keep用于存放，NMS后剩余的方框(保存所有结果框的索引值)
    keep = []
    while order.numel() > 0:
        if order.numel() == 1: #只剩下一个时候直接放进去，例子中只剩下3
            keep.append(order.item())
            break
        else:
            # 置信度最高的索引
            i = order[0].item()
            # keep保留的是索引值，不是具体的分数。
            keep.append(i) # 添加本次置信度最高的boundingbox的index；在上面的例子中，第一次加入‘0’
        # 计算最大得分的bboxs[i]与其余各框的IOU
        #第一次，iou为tensor([0.0000, 0.7368, 0.5454])
        iou = box_iou(bboxs[i, :].reshape(-1, 4),
                      bboxs[order[1:], :].reshape(-1, 4)).reshape(-1)
        # 保留iou小于阈值的剩余bboxs,iou小表示两个box交集少，可能是另一个物体的框，故需要保留
        idx = torch.nonzero((iou <= threshold)).reshape(-1)  # 返回非零元素的索引
        # 待处理boundingbox的个数为0时，结束循环
        if idx.numel() == 0:
            break
        # 把留下来框在进行NMS操作
        # 这边留下的框是去除当前操作的框，和当前操作的框重叠度大于thresh的框
        # 因为处理的时候是对tensor([0.0000, 0.7368, 0.5454])进行处理的，去掉了第一个最大的框子
        #最后返回的时候，要把第一个位置给他加上去。也就是idx+1后挪一位
        order = order[idx + 1] #iou小于阈值的框
    return torch.tensor(keep,device=bboxs.device)
```
Andrew Ng 建议每种对象分别进行 NMS,但是这里的话，应该是先做了一个取类别概率最大值的操作，直接对整个进行 NMS，没有分别对每种对象进行NMS。

**将非极大值抑制应用于预测边界框**


```python
# cls_probs = torch.tensor([[0] * 4, # 背景的预测概率
# [0.9, 0.8, 0.7, 0.1], # 狗的预测概率
# [0.1, 0.2, 0.3, 0.9]]) # 猫的预测概率
#anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
# [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
#offset_preds = torch.tensor([0] * anchors.numel()).reshape(-1, 4) #这边为了方便 偏移量都设为0
# _______________________________________________________________________________________________
# 将非极大值抑制应用于预测边界框
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.0099):
    """使用非极大值抑制来预测边界框。"""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)
    # 保存最终的输出
    out = []
    for i in range(batch_size):
        # 预测概率和预测的偏移量
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
		#tensor([[0, 0, 0, 0],
		# [0, 0, 0, 0],
		# [0, 0, 0, 0],
		# [0, 0, 0, 0]])
        # 非背景的概率及其类别索引
        #torch.max(input, dim)，dim=0代表每列的最大值
        #函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。
        conf, class_id = torch.max(cls_prob[1:], 0)
        #tensor([0.90, 0.80, 0.70, 0.90]) tensor([0, 0, 0, 1])

        # 预测的边界框坐标
        predicted_bb = offset_inverse(anchors, offset_pred)
        # 对置信度进行排序并返回其索引[0,3,1,2]
        all_id_sorted = torch.argsort(conf, dim=-1, descending=True)

        keep = nms(predicted_bb, conf, nms_threshold)  # 非极大值抑制结果 [0,3]
        # 找到所有的 non_keep 索引，并将类设置为背景
        non_keep = []
        for i in range(all_id_sorted.numel()):
            res = all_id_sorted[i] in keep
            if not res:
                non_keep.append(all_id_sorted[i].item())
        non_keep = torch.tensor(non_keep) # [1,2]
        # 将类设置为背景-1
        class_id[non_keep] = -1 # tensor([ 0, -1, -1, 1])
        # 对应的类别标签
        class_id = class_id[all_id_sorted] #tensor([ 0, 1, -1, -1])
        # 排序,conf为tensor([0.90, 0.90, 0.80, 0.70])
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]

        # `pos_threshold` 是一个用于非背景预测的阈值
        below_min_idx = (conf < pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]

        pred_info = torch.cat(
            (class_id.unsqueeze(1), conf.unsqueeze(1), predicted_bb), dim=1)
'''
tensor([[[ 0.0000, 0.9000, 0.1000, 0.0800, 0.5200, 0.9200],
 [ 1.0000, 0.9000, 0.5500, 0.2000, 0.9000, 0.8800],
 [-1.0000, 0.8000, 0.0800, 0.2000, 0.5600, 0.9500],
 [-1.0000, 0.7000, 0.1500, 0.3000, 0.6200, 0.9100]]])
'''
        out.append(pred_info)
    return torch.stack(out)
```
把图像画出来


```python
# 读取图片
img = plt.imread("-")
h,w = img.shape[:2]
bbox_scale = torch.tensor((w,h,w,h))
fig = plt.imshow(img)
# 删除 -1 （背景）类的预测边界框，输出由非极大值抑制保存的最终预测边界框。
for i in output[0].detach().numpy():
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)
plt.show()
```
![](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/v2-79cab5eb1479716c25e6efad29960e7e_r.jpg)

---

感觉这章还是挺难的，大概走了两遍的样子，理解起来稍微好一点了。当然在实际的操作过程中，不需要自己写这些函数，可以直接调用‘from mxnet import contrib’(简单的实现，等后续需要使用的时候再详细看看)。

参考资料
----

[《动手学深度学习》13.4锚框_whut_52xj的博客-CSDN博客_深度学习 锚框](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_48192326/article/details/119461890%3Futm_source%3Dapp%26app_version%3D4.21.0%26utm_source%3Dapp)(这个写的很好，很详细)

[锚框：Anchor box综述](https://zhuanlan.zhihu.com/p/63024247) （主要了解了训练的过程）

[锚框（anchor box）理解和代码实现](https://zhuanlan.zhihu.com/p/450451509)（生成锚框的写法和视频中不同，我认为比视频中容易理解）

