{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import torch,random,re,collections,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "with open(\"./data/timemachine.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [re.sub(\"[^A-Za-z]+\", \" \", line).strip().lower() for line in lines]\n",
    "\n",
    "\n",
    "# 词元化\n",
    "def tokenize(strings, token=\"word\"):\n",
    "    if token == \"word\":\n",
    "        return [str.split() for str in strings]\n",
    "    elif token == \"char\":\n",
    "        return [list(str) for str in strings]\n",
    "    else:\n",
    "        print(\"Wrong!\")\n",
    "\n",
    "\n",
    "tokens = tokenize(lines, token=\"word\")\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词元的频率\n",
    "def count_corpus(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "# 词表\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens=None, min_freq=0, reversed_tokens=None) -> None:\n",
    "        if tokens == None:\n",
    "            tokens = []\n",
    "        if reversed_tokens == None:\n",
    "            reversed_tokens = []\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=self.cmp, reverse=True)\n",
    "        self.idx_to_token = [\"<unk>\"] + reversed_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break  # 后面的不加入到词表中\n",
    "            if token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.idx_to_token)\n",
    "                self.idx_to_token.append(token)\n",
    "\n",
    "    def cmp(self, x):\n",
    "        return x[1]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.to_tokens(indice) for indice in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n",
      "0\n",
      "文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "文本: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])\n",
    "print(vocab['ads'])\n",
    "for i in [0, 10]:\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cropus(max_tokens=-1, file_name=\"timemachine\", pattern=\"word\"):\n",
    "\n",
    "    with open(f\"./data/{file_name}.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [re.sub(\"[^A-Za-z]+\", \" \", line).strip().lower() for line in lines]\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocabulary(tokens=tokens, min_freq=0)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2261),\n",
       " ('i', 1267),\n",
       " ('and', 1245),\n",
       " ('of', 1155),\n",
       " ('a', 816),\n",
       " ('to', 695),\n",
       " ('was', 552),\n",
       " ('in', 541),\n",
       " ('that', 443),\n",
       " ('my', 440)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenize(read_time_machine())\n",
    "# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起\n",
    "corpus = [token for line in tokens for token in line]\n",
    "vocab = Vocabulary(corpus)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, vocab = load_cropus(pattern='word')\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps, random_offset=\"True\"):\n",
    "    if random_offset:\n",
    "        a = random.randint(0, num_steps - 1)\n",
    "        # 如果偏移量大于时间步x，则偏移量中的x可以作为新的时间步\n",
    "    else:\n",
    "        a = 0\n",
    "    print(f'random offset: {a}')\n",
    "    # update corpus\n",
    "    corpus = corpus[a:]\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    initial_indices=[i*num_steps for i in range(num_subseqs)]\n",
    "    random.shuffle(initial_indices)\n",
    "    num_batches=num_subseqs//batch_size\n",
    "    for i in range(0,num_batches*batch_size,batch_size):\n",
    "        initial_indices_per_batch=initial_indices[i:i+batch_size]\n",
    "        X=[corpus[j:j+num_steps]for j in initial_indices_per_batch]\n",
    "        Y=[corpus[j+1:j+num_steps+1]for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X),torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random offset: 2\n",
      "X:  tensor([[12, 13, 14, 15, 16],\n",
      "        [ 7,  8,  9, 10, 11]]) \n",
      "Y: tensor([[13, 14, 15, 16, 17],\n",
      "        [ 8,  9, 10, 11, 12]])\n",
      "X:  tensor([[27, 28, 29, 30, 31],\n",
      "        [ 2,  3,  4,  5,  6]]) \n",
      "Y: tensor([[28, 29, 30, 31, 32],\n",
      "        [ 3,  4,  5,  6,  7]])\n",
      "X:  tensor([[17, 18, 19, 20, 21],\n",
      "        [22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[18, 19, 20, 21, 22],\n",
      "        [23, 24, 25, 26, 27]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0,  1,  2,  3,  4],\n",
      "        [17, 18, 19, 20, 21]]) \n",
      "Y: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [18, 19, 20, 21, 22]])\n",
      "X:  tensor([[ 5,  6,  7,  8,  9],\n",
      "        [22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [23, 24, 25, 26, 27]])\n",
      "X:  tensor([[10, 11, 12, 13, 14],\n",
      "        [27, 28, 29, 30, 31]]) \n",
      "Y: tensor([[11, 12, 13, 14, 15],\n",
      "        [28, 29, 30, 31, 32]])\n"
     ]
    }
   ],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  # @save\n",
    "    offset = random.randint(0, num_steps - 1)\n",
    "    offset = 0\n",
    "    corpus = corpus[offset:]\n",
    "    num_tokens = ((len(corpus) - 1 )// batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[0:num_tokens]).reshape(batch_size, -1)\n",
    "    Ys = torch.tensor(corpus[1 : num_tokens + 1]).reshape(batch_size, -1)\n",
    "    num_batches = num_tokens // (batch_size * num_steps)\n",
    "    for i in range(0, num_batches):\n",
    "        X = Xs[:, i * num_steps : (i + 1) * num_steps]\n",
    "        Y = Ys[:, i * num_steps : (i + 1) * num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print(\"X: \", X, \"\\nY:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(\n",
    "        self, batch_size, num_steps, use_random_iter, max_tokens, pattern=\"word\"\n",
    "    ) -> None:\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_cropus(max_tokens, pattern=pattern)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(\n",
    "    batch_size, num_steps, use_random_iter=False, max_tokens=10000\n",
    "):\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size,\n",
    "        num_steps,\n",
    "        use_random_iter=use_random_iter,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "%matplotlib inline\n",
    "import math,torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "batch_size,num_steps=32,35\n",
    "train_iter,vocab=load_data_time_machine(batch_size,num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs,num_outputs=vocab_size,vocab_size\n",
    "\n",
    "    W_xh=torch.randn((num_inputs,num_hiddens),device=device)\n",
    "    W_hh=torch.randn((num_hiddens,num_hiddens),device=device)\n",
    "    b_h=torch.zeros(num_hiddens,device=device)\n",
    "    W_hq=torch.randn((num_hiddens,num_outputs),device=device)\n",
    "    b_q=torch.zeros(num_outputs,device=device)\n",
    "    params=[W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad=True\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size,num_hiddens,device):\n",
    "    return torch.zeros((batch_size,num_hiddens),device=device)\n",
    "    # 加逗号是使用元组，没有逗号会被当做是(a)=a的表达式\n",
    "def rnn(inputs, state, params):\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H = state\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        h = torch.tanh(torch.matmul(x, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        o = torch.matmul(h, W_hq) + b_q\n",
    "        outputs.append(o)\n",
    "    # o的形状：(num_step*batch_size，词表大小)\n",
    "    return torch.cat(outputs, dim=0), H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "print(try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:\n",
    "    def __init__(\n",
    "        self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn\n",
    "    ):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(self.vocab_size, self.num_hiddens, device)\n",
    "        self.init_state = init_state\n",
    "        self.forward_fn = forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 2, torch.Size([512]), 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0],try_gpu())\n",
    "Y, new_state = net(X.to(try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape,net.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix,num_preds,net,vocab,device):\n",
    "    outputs=[vocab[prefix[0]]]\n",
    "    state=net.begin_state(batch_size=1,device=device)\n",
    "    for y in prefix[1:]:\n",
    "        _,state=net(torch.tensor([outputs[-1]],device=device).reshape((1,1)),state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y,state=net(torch.tensor([outputs[-1]],device=device).reshape((1,1)),state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    # return ''.join(*vocab.to_tokens([outputs]))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller k<unk>ssssssssssssssssssssssssssss\n"
     ]
    }
   ],
   "source": [
    "print(predict('time traveller ', 30, net, vocab,try_gpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state=None\n",
    "    time_start=time.time()\n",
    "    for X,Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state=net.begin_state(batch_size=X.shape[0],device=device)\n",
    "        else:\n",
    "            if isinstance(net,nn.Module()) and not isinstance(state,tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y=Y.T.reshape(-1)\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        y_hat,state=net(X,state)\n",
    "        l=loss(y_hat,y.long()).mean()\n",
    "        if isinstance(updater,torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net,1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater(batch_size=1)\n",
    "        # total_loss=l*y.numel()\n",
    "        time_use=time.time()-time_start\n",
    "        return math.exp(l),y.numel()/time_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    updater=torch.optim.SGD(net.parameters(),lr)\n",
    "    # updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(\n",
    "            net, train_iter, loss, updater, device, use_random_iter\n",
    "        )\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict(\"time traveller\", 50, net, vocab, device))\n",
    "            print(f\"epoch {epoch+1}, ppl {ppl}\")\n",
    "    print(f\"困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}\")\n",
    "    print(predict(\"time traveller\", 50, net, vocab, device))\n",
    "    print(predict(\"traveller\", 50, net, vocab, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs, lr = 500, 1\n",
    "# train(net, train_iter, vocab, lr, num_epochs, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN concise\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "num_hiddens=256\n",
    "rnn_layer=nn.RNN(len(vocab),num_hiddens)\n",
    "state=torch.zeros((1,batch_size,num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size) -> None:\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            return torch.zeros(\n",
    "                (\n",
    "                    self.num_directions * self.rnn.num_layers,\n",
    "                    batch_size,\n",
    "                    self.num_hiddens,\n",
    "                ),\n",
    "                device=device,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size,\n",
    "                        self.num_hiddens,\n",
    "                    ),\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.zeros(\n",
    "                    (\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size,\n",
    "                        self.num_hiddens,\n",
    "                    ),\n",
    "                    device=device,\n",
    "                ),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellereyyyyyyyyy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(try_gpu())\n",
    "predict(\"time traveller\", 10, net, vocab, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller                                                  \n",
      "epoch 10, ppl 18.564595587381607\n",
      "time traveller                                                  \n",
      "epoch 20, ppl 16.91033488729284\n",
      "time traveller th  a   a  a   a  a   a  a   a  a   a  a   a  a  \n",
      "epoch 30, ppl 14.501900209752568\n",
      "time traveller ao  an  an  an  an  an  an  an  an  an  an  an  a\n",
      "epoch 40, ppl 12.559301541730122\n",
      "time traveller ao  ae the  ao  ae the  ao  ae the  ao  ae the  a\n",
      "epoch 50, ppl 11.240032706620338\n",
      "time traveller an in the the the the the the the the the the the\n",
      "epoch 60, ppl 10.00452175802084\n",
      "time traveller the the the t the the the the the the the the the\n",
      "epoch 70, ppl 8.966000684356063\n",
      "time traveller ar he tant the  ar ant the  ar ane the  ar se tan\n",
      "epoch 80, ppl 8.55381858946517\n",
      "time traveller an ho s io s an in s an he he the he the s than s\n",
      "epoch 90, ppl 7.409951022602628\n",
      "time traveller ae wer ae sar an in the the and ane the  al ine t\n",
      "epoch 100, ppl 6.919818847756392\n",
      "time traveller the shes ne the snest movelest of the the the sne\n",
      "epoch 110, ppl 6.739713367392955\n",
      "time traveller thav sumend the ger and the ger and chov canthes \n",
      "epoch 120, ppl 6.084297484340836\n",
      "time traveller aith a sllll ih wanlligh aa cang tha ill ha halli\n",
      "epoch 130, ppl 5.478169944042467\n",
      "time traveller wh we mav has anth go eurathe bure mathe indir ta\n",
      "epoch 140, ppl 5.218368702830024\n",
      "time traveller and the bat and and an tant an s an an wons an s \n",
      "epoch 150, ppl 5.247131699535452\n",
      "time traveller wh s aus ims ins ans ans ans ans ans ans ans ans \n",
      "epoch 160, ppl 5.253924092359196\n",
      "time traveller th t aleler the one comed accons thes nou t acone\n",
      "epoch 170, ppl 4.403703781138792\n",
      "time traveller whove iand at ha the tarthe ane ne time ler ant u\n",
      "epoch 180, ppl 4.19054006094052\n",
      "time traveller th w aland the that ne thot ne t ma tha ills you \n",
      "epoch 190, ppl 3.8716429108604404\n",
      "time traveller withea illly have the gathangerermale nacoflmint \n",
      "epoch 200, ppl 3.2947423683298114\n",
      "time traveller th t anelyth th t ar he t e sur the cant ne t mo \n",
      "epoch 210, ppl 2.90446616434946\n",
      "time traveller with a s ch s he bres emov sou n as nf t e t bup \n",
      "epoch 220, ppl 2.4555481132542782\n",
      "time travelleryou can srow tha d asc ns the ble ne tan an ind an\n",
      "epoch 230, ppl 2.164417670641925\n",
      "time travelleryou can sailendatiur thatint thithe batclest wh wa\n",
      "epoch 240, ppl 2.0646616171891328\n",
      "time travelleryou cha cione tarehe taicenstroch lo bres toat las\n",
      "epoch 250, ppl 1.8745329896238339\n",
      "time travelleryou can show blask ws thascelush wo clasg mave wec\n",
      "epoch 260, ppl 1.6022412811746012\n",
      "time traveller wu can sres bmack as of the fae h it eubs wh in w\n",
      "epoch 270, ppl 1.4671160538545318\n",
      "time traveller with a slight accessther abllrsithe mich ione t a\n",
      "epoch 280, ppl 1.3797424496496429\n",
      "time traveller with a slight accessinof tist mave dimonemescan a\n",
      "epoch 290, ppl 1.129894285722056\n",
      "time traveller with a slight accessinof tist mave dimbyan iom fi\n",
      "epoch 300, ppl 1.1105665654903811\n",
      "time traveller with a slight accessinof tist mave dimen aone tor\n",
      "epoch 310, ppl 1.0982665653682628\n",
      "time traveller with a slight accessinof tist mave wi ha fensenda\n",
      "epoch 320, ppl 1.0891556839293404\n",
      "time traveller with a slight accessinof tist mavesnthon hed and \n",
      "epoch 330, ppl 1.0820031393446492\n",
      "time traveller with a slight accessinof tist mavesnthon hed and \n",
      "epoch 340, ppl 1.0762295177102854\n",
      "time traveller with a slight accessinof tist mavesnt ba len co h\n",
      "epoch 350, ppl 1.0714970620751727\n",
      "time traveller with a slight accessinof tistnmattha chat ee hed \n",
      "epoch 360, ppl 1.067555402802911\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 370, ppl 1.0642111580124982\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 380, ppl 1.0613257800649072\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 390, ppl 1.0588028297316534\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 400, ppl 1.0567575570168657\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 410, ppl 1.0548451575654618\n",
      "time traveller with a slight accessinof tistnmattha chat ee hey \n",
      "epoch 420, ppl 1.0529584205016447\n",
      "time traveller with a slight accessinot tist mave wi whave been \n",
      "epoch 430, ppl 1.0513220179953096\n",
      "time traveller with a slight accessinot tist mave wi whavee stho\n",
      "epoch 440, ppl 1.0498526700190536\n",
      "time traveller with a slight accessinot tist mave wi whavee stho\n",
      "epoch 450, ppl 1.0485199887049603\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "epoch 460, ppl 1.047305663579167\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "epoch 470, ppl 1.0461956735108937\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "epoch 480, ppl 1.0451783085713187\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "epoch 490, ppl 1.0442435287257763\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "epoch 500, ppl 1.0433826561455815\n",
      "困惑度 1.0, 117396.5 词元/秒 cuda:0\n",
      "time traveller with a slight accessinot tist mave wi whavee sen \n",
      "traveller with a slight accessinot tist mave wi whavee stho\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train(net, train_iter, vocab, lr, num_epochs, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
